{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ***Legal Analytics - BAIL Prediction***\n",
    "\n",
    "#### **TFIDF + IndicBERT Model**\n",
    "\n",
    "#### *(Without Quantum)*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Necessary imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from evaluate import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Pretrained IndicBERT Tokenizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Pretrained IndicBERT model initialization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"ai4bharat/indic-bert\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Checking for GPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (CUDA or CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### *Loading Dataset*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_json(r\"D:/BP/data/full_data_train.json\")\n",
    "test_df = pd.read_json(r\"D:/BP/data/full_data_test.json\")\n",
    "\n",
    "# For hyperparameter search, use a sample of 10% of the data\n",
    "hp_train_df = train_df.sample(frac=0.1, random_state=42)\n",
    "hp_test_df = test_df.sample(frac=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Custom Dataset class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        # Extracting text from ranked-sentences\n",
    "        self.df[\"text\"] = self.df[\"ranked-sentences\"].progress_apply(\n",
    "            lambda x: \" \".join(x[:10]) if isinstance(x, list) else \" \".join(eval(x)[:10])\n",
    "        )\n",
    "        self.df[\"label\"] = self.df[\"label\"]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Get the text for the current index\n",
    "        model_input = self.df['text'][idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded_sent = self.tokenizer.encode_plus(\n",
    "            text=model_input, \n",
    "            add_special_tokens=True,       \n",
    "            max_length=512,                  \n",
    "            padding='max_length',          \n",
    "            return_attention_mask=True, \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        input_ids = torch.tensor(encoded_sent.get('input_ids'))\n",
    "        attention_mask = torch.tensor(encoded_sent.get('attention_mask'))        \n",
    "        label = torch.tensor(self.df['label'][idx])\n",
    "        \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}\n",
    "    \n",
    "    def print_samples(self, num_samples=2):\n",
    "        \"\"\"Prints a few samples from the dataset.\"\"\"\n",
    "        for i in range(num_samples):\n",
    "            sample = self[i]\n",
    "            print(f\"Sample {i + 1}:\")\n",
    "            print(f\"Input IDs: {sample['input_ids'][:10]}\")  # Print first 10 input IDs\n",
    "            print(f\"Attention Mask: {sample['attention_mask'][:10]}\")  # Print first 10 attention mask values\n",
    "            print(f\"Label: {sample['label']}\")\n",
    "            print(\"-\" * 20)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Creating Datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LegalDataset(train_df, tokenizer)\n",
    "test_dataset = LegalDataset(test_df, tokenizer)\n",
    "hp_train_dataset = LegalDataset(hp_train_df, tokenizer)\n",
    "hp_test_dataset = LegalDataset(hp_test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print samples from the training dataset\n",
    "print(\"Training Dataset Samples:\")\n",
    "train_dataset.print_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### *Analysis*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = \"label\", hue = \"label\",data = train_df, palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = \"label\", hue = \"label\",data = test_df, palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = len(train_df)\n",
    "n2 = len(test_df)\n",
    "print(\"\\n\")\n",
    "print(\"=\"*50)\n",
    "print(f\"No of Samples in Train Dataset : {n1}\")\n",
    "print(f\"No of Samples in Test Dataset  : {n2}\")\n",
    "print(f\"Total                          : {n1 + n2}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\"*50)\n",
    "print(\"Distribution in Train Dataset\")\n",
    "train_granted, train_dismissed = train_df[\"label\"].value_counts()\n",
    "print(f\"Bail Granted   : {train_granted}\")\n",
    "print(f\"Bail Dismissed : {train_dismissed}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\"*50)\n",
    "print(\"Distribution in Test Dataset\")\n",
    "test_granted, test_dismissed = test_df[\"label\"].value_counts()\n",
    "print(f\"Bail Granted   : {test_granted}\")\n",
    "print(f\"Bail Dismissed : {test_dismissed}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"_\"*50)\n",
    "print(f\"Total Granted   : {train_granted + test_granted}\")\n",
    "print(f\"Total Dismissed : {train_dismissed + test_dismissed}\")\n",
    "print(\"_\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Shows Class Imbalance**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fixing Metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "f1_metric = load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Metric computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"micro\")\n",
    "    return {'accuracy': accuracy[\"accuracy\"], 'f1-score': f1[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ***Hyperparameter Search***\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search space for Optuna\n",
    "def my_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.005, 0.05),\n",
    "        \"adam_beta1\": trial.suggest_float(\"adam_beta1\", 0.75, 0.95),\n",
    "        \"adam_beta2\": trial.suggest_float(\"adam_beta2\", 0.99, 0.9999),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(\"TFIDF-INDIC\", \"output\"),\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(\"TFIDF-INDIC\", \"logs\"),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=250,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1-score\",\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer for hyperparameter search\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=hp_train_dataset,\n",
    "    eval_dataset=hp_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter search\n",
    "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\", hp_space=my_hp_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garbage collection to free memory after hyperparameter tuning\n",
    "del trainer\n",
    "del training_args\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ***Model Training***\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with best hyperparameters\n",
    "print(\"Starting final training...\")\n",
    "\n",
    "# Reload training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(\"TFIDF-INDIC\", \"output\"),\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(\"TFIDF-INDIC\", \"logs\"),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=250,\n",
    "    save_strategy='epoch',\n",
    "    learning_rate = 0.00001,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1-score\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best hyperparameters and start training\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(r\"D:/BP/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
